25.03.08.
beautifulsoup과 request를 이용하여 간단한 크롤링 기법 구현(정적으로 구성된 news 페이지)

25.03.10 ~ 25.03.11.
구현한 크롤링 코드를 스케쥴러에 의해 작동되도록 변경
ABC 라이브러리를 이용하여 부모, 자식 클래스로 세분화하여 인터페이스 구현
main.py에서 각 라이브러리의 __init__을 호출함으로써 객체를 생성하는 방식으로 변경(OOP)

25.03.12. ~ 25.03.13.
각 크롤러가 스레드에서 실행되도록 변경
html 구조를 다시 분석하여 선택자 최적화
> 크롤링 할 각 요소를 따로 부르는 방식에서 큰 컨테이너를 먼저 부르고, 하위 요소들을 추출하는 방식으로 변경
애널리스트 리포트 데이터 크롤링 구현 완료

25.03.13.
html의 각 요소를 main에서 가져오는지, content에서 가져오는지 json 파일만 수정하면 유동적으로 작동하게 변경
스케쥴러도 json 파일로 관리하도록 변경
제무제표, 거시경제지표 크롤링 추가

25.03.17.
크롤링 데이터 tag 추가(DB에 넘길때 사용)
error fix

25.03.19.
주가 데이터 크롤링 추가

25.03.25.
각 크롤링 함수 데이터 구조 통일
traceback 예외처리 구현

25.03.26.
Secretary(DB저장 코드)추가
model 및 handler를 이용하여 구현

25.03.30.
크롤링 대상(선택자) 함수 구분에 핸들러 방식 적용
DB 기본키 오류 -> 복합키로 변경하여 보장
traceback 예외처리 방식을 전체 코드를 대상으로 변경
DB 저장시 rollback 오류 -> null값 가능성이 있는 데이터의 속성 변경
rollback 오류2 (외래키 제약조건) -> financial 데이터 insert시 flush로 바뀐 상태를 불러오지 않아서 외래키를 불러오지 못했음. 수정
DB 저장 코드 테스트 완료

25.04.01.
재무제표 데이터 DB 수정(프론트 요청)
not null 데이터 구분, 뷰 수정 / 필수 데이터 fallback 구현
traceback 예외처리 패키지 구분
기본키 생성 방식을 크롤링 데이터 해싱 방식으로 변경(중복 방지)
주가 데이터 symbol 압축 및 병렬처리 구현

25.04.05. ~ 25.04.06.
-- crawling DB
실시간성을 고려하여 주요 주식 종목은 1분, 나머지는 5분 15분으로 차등적으로 데이터를 크롤링
예외 코드를 미리 작성한 뒤 처리하는 방식으로 변경
-- assistAPIServer
백엔드 실전(controller service 구조, entity repository dto 구조)
change값 등 계산이 필요한 건 service 같이 로직이 실행되는 곳에서 처리 이해
dto 등은 데이터 형식만 지정하는 것 이해
전역 예외처리 및 예외타입 지정하여 프론트로 보내는 구조 이해
멀티 소스 기반에서 데이터를 어디서 가져오는지 지정을 안해서 에러가 났음
멀티 소스 기반 구조의 이해
-- github action
브랜치 보호규칙에 맞게 pull request 하는법(CI test 통과)
git 기록, 복구 등 git 명령어 학습
lint, pytest 조건에 따라서 실패한 request에 대한 시행착오
-- 협업 관련
프론트의 주식 종목 검색기능에 사용할 symbol - name 매칭 테이블의 요구를 반영하여 테이블 추가
주가 비율, 변화율 등 프론트에서 요청한 데이터를 처리하기 위해 크롤링 및 백엔드에 로직을 반영함
뷰 테이블에 프론트에서 필요한 데이터가 누락되어 해당 데이터를 추가하고, 뷰 구조를 재정의함
-- DB 관련
클라우드 지원이 시작되기 전에 로컬 서버에서 구축하기로 결정하여 assistAPIServer DB 구축을 시작함
데이터 insert시 uuid를 생성하고, binary 형식으로 변환하는 방식으로 메모리 및 성능의 향상을 기대할 수 있다는 것을 공부함
해당 내용을 반영하여 저장시에는 binary로, 뷰에는 다시 binary를 uuid로 변환하는 방식으로 구현함

25.04.28 ~ 25.05.02
-- 크롤링
컴파일 실패 - 크롤링 대상 사이트의 구조 변경으로 인한 실패
수정 - 코드 내용의 수정 없이, 기존에 구현한 대로 selector 파일만 수정하여 대응 성공
DB 저장시 여러 데이터 프레임을 넣다가 한번 롤백되면 나머지 데이터들에도 영향이 갔음
수정 - 한 데이터가 잘못되어서 rollback되어도 그 내용만 스킵하고 fail log를 기록한 뒤 다음 df로 넘어가는 방식으로 수정함
28일 회의 중 현재의 로깅 시스템으로는 디버깅이 힘들다는 사실을 알게됨
수정 - 로깅 시스템을 만들어서 고정된 prefix를 통해 시각적으로 보기 편한 로그를 남기고, 파일로까지 저장하는 방식으로 수정함
28일 회의 중 디버깅시 필요한 세부 설정 변경들이 불편하다는 것을 알게됨
수정 - 전역 설정 파일을 별도로 두고, 내부 로직에서는 설정 파일의 내용을 담은 싱글톤 객체를 참조하는 방식으로 변경함
또한, 설정파일 변경시 reload하는 로직을 두어 서버에서 돌아가는 도중에 설정파일을 변경하여도 대응하도록 설정함
-- assistAPIServer
28일, 30일 협업 중 재무제표 데이터를 백엔드에서 프론트엔드로 보내는 과정에서 사용자가 불편하게 느낄만한 지연이 발생함
원인 - 매 요청마다 새로운 쿼리가 발생하여 DB에 부담이 되고, 페이지를 구하는 로직이 N + 1 문제가 발생하였음
수정 - 페이징 방식을 슬라이싱 방식으로 구현하여 최적화 하고, DB 쿼리 방식을 offset에서 lastticker를 구하는 방식으로 변경하여 최적화함
재무제표 데이터를 프론트로 보낼 때, 한 페이지에 3개의 데이터가 포함되어야하는데, DB 데이터가 부족할때 3개보다 적은 데이터를 보내는 문제가 생김
수정 - 데이터 검사 로직을 추가하여 항상 3개의 데이터를 보낼 수 있도록 처리함
협업 - 프론트의 요청에 따라 시스템 테스트시 변경해야하는 파라미터를 하드코딩으로 수정해야하는 불편함이 생겼음
수정 - 전역 설정파일을 추가하여 처리함

25.05.03. ~ 25.05.16.
-- 크롤러
디버깅 용이성을 위하여 각 크롤러의 on off를 설정파일을 통해 작동하도록 변경함
협업 - AI 담당측의 요청으로 분석에 필요한 데이터를 최대한 수집하기 위해 재무제표 데이터의 수집 방식을 개선함. 필요 데이터 누적시 재시도 로직 구현
모든 크롤링마다 재수집하던 분기데이터를 캐싱을 통해 기존 데이터를 활용하는 방식으로 변경하여 성능 향상을 도모함
주가 변화량 계산 위치를 크롤링 시점으로 변경하여 기존 DB 쿼리로 처리하던 부분을 개선함
컴파일 실패 - 주식 정보를 수집하는 yfinance 라이브러리의 오류가 발생함
수정 - 라이브러리의 업데이트 전까지 일시적으로 세션을 강제 주입하는 방식으로 해결함
수정2 - 이후 라이브러리 업데이트를 코드에 적용하고, 오류 재발 방지를 위해 세션 주입 방식을 유지함
디버깅 용이성을 위해 각 크롤러마다 기록되는 로그 폴더를 분리함
---- 스케쥴러
각 크롤러의 특성에 따라 스케쥴 세분화
스케쥴 세분화에 따른 별도의 스케쥴러를 구성
기존 sleep 방식에서 ttl 캐쉬를 사용하는 방식으로 변경함
스케쥴이 정의된 json을 읽고, 해당 파일을 기준으로 ttl을 설정
전역 설정에서 테스트 모드가 켜진 경우는 고정된 ttl을 사용하도록 구현함
---- 로거
DEBUG의 경우 테스트 모드인 경우만 출력되도록 변경함


25.05.17 ~ 25.05.23
--- 크롤러
1. 주가 정보 크롤링 변경사항
- 기존에 한 클래스에서 분기데이터, 일간데이터를 한번에 가져오던 방식에서 분기, 일반 수집 로직을 분리하여 자식클래스로 구성함(가독성, 유지보수성, 모듈화)
- SEC의 cik request를 통해 가져온 데이터에 오류(cik와 회사티커 매칭 오류)가 있어서 전부 yfinance 라이브러리를 사용하는 방식으로 변경함
- 프론트측의 종목 테이블 페이지 추가에 대응하여 기존 최근 1분봉 거래량만 수집하던걸 누적 거래량을 수집하도록 변경함
- 크롤러 구조 변경에 따른 형변환 등의 오류에 대응함
- 기존의 분기, 일간 데이터의 확인을 파일 혹은 메모리 캐쉬에 의존하던 방식에서 전부 DB 저장 방식으로 변경하여 안정성을 높임
2. Notifier(AI 백엔드에 분석을 요청하는 클래스)
- 뉴스 및 리포트, 재무제표 데이터에 대해 AI 분석을 소켓 통신으로 요청하는 로직을 구현해야 했음
- 분석에 필요한 데이터는 뉴스, 리포트, 재무제표에 더해 해당 종목의 주가 데이터도 요구하는 방식
- 그러나 크롤러는 각 데이터가 스레드 형식으로 진행되고 끝나는 시기가 불규칙적이라 기존 로직에 소켓 통식을 삽입하는 방식에 한계가 있음
- 처음에는 크롤러에서 한번에 처리하고자 소켓을 연결하기 전에 필요한 데이터를 검사하고 대기하는 로직을 시도
- 이 방식은 데이터 수집에 너무 많은 시간 손실이 발생한다는 것을 알게됨
- 따라서 아예 크롤러와 Notifier를 분리하여 따로 작동하는 방식으로 변경함
- 크롤러는 데이터만 수집하고, Notifier는 수집된 데이터를 DB 쿼리를 통해 가져오고, crawling_id를 통해 응답 결과를 적절하게 저장하는 방식을 택했음
- Notifier는 크롤러와 별개로 주기적으로 작동하기 때문에 데이터가 다 수집되지 않은 경우는 다음 사이클에 진행되는 방식으로 데이터 수집 타이밍 문제를 해결함
- 테스트 도중 뉴스,레포트 분석과 재무제표 분석의 차이점으로 인해 분석결과를 저장하는 로직의 분리가 필요해짐
- 부모 클래스에서 한번에 분기하여 저장하던 로직을 각 자식의 성향에 맞게 처리하기 위해 추상 클래스로 구현하여 자식 클래스에서 완성하는 방식으로 변경함
3. 구조 변경에 따른 CI 오류 대응
- Notifier 추가에 따른 main함수 변경사항(기존 크롤러만 run하던걸 두 클래스를 병렬 실행하는 방식으로 변경)으로 인해 CI test에 오류가 발생함
- import circulation 등의 문제를 해결하고 클래스의 run 함수가 실제로 실행되는 부분을 패키지 init 파일에 구성하여 해결함
4. 기타
- 재무제표 데이터를 저장할때 기존의 db.add 방식이 너무 오래 걸린다는 것을 알게 되어 bulk_insert 방식으로 변경하여 속도를 향상함
- 주가 정보 및 financial 데이터를 저장할때 target_url이 null로 되어있었는데, 적절한 데이터를 추가하여 null 칼럼의 사용을 줄이고 디버깅 용이성을 늘림
- log_summury 시점에서 다른 크롤링 클래스에서 발생한 error count를 출력하는 오류가 생김
- error count가 인스턴스 변수가 아닌 클래스 변수로 되어있었고, 이를 수정하여 해결함 
--- DB
- 크롤러 및 프론트엔드 변경사항에 대응하여 Notifier, Stock 크롤러 구조에 맞는 테이블 수정 및 뷰 생성
--- 협업
- AI 분석에 필요한 뉴스 데이터는 하나의 기사 : 하나의 종목 (1:1) 방식
- 그러나 실제 뉴스는 하나의 기사 : 여러 종목(1:n) 형식
- 따라서 요청 단계에서 여러 ticker를 분리하여 각각 분석 결과를 받고, DB에 분리 저장하는 방식으로 수정
- 이에따라 웹의 프론트, 백엔드에서 이를 처리해야하는 상황이 발생함
- 기존 view 구조를 최대한 유지하는 방향으로 DB에서 분석 결과를 concat하여 수정이 적은 방향으로 진행함
- 프론트의 뉴스 페이지에서 하나의 기사에 하나의 분석결과가 뜨던 방식을 여러 종목의 분석결과를 띄우는 방식으로 수정하기로 함

25.05.24. ~ 25.05.30.
1. 크롤러
- 웹사이트 크롤링 시 잘못된 선택자로 인해 불필요한 데이터 수집하던 문제를 수정
- 사용자 편의를 위해 영어 기사 데이터 -> 한글 번역 기능 추가
- 같은 뉴스가 여러 사이트에서 크롤링되어 중복되어 수집되는 문제 수정(title 기준으로 중복 처리)
- 로그가 한 파일로만 저장되어 디버깅이 어려워서, 시간 단위로 끊어서 저장으로 수정
- 분석에 필요한 데이터를 미리 검증한 후 요청하는 로직 추가
- 소켓 통신 간 에러 디버깅을 위한 로깅 추가
- 데이터 추가 수집으로 발생하는 연간 주가 데이터의 중복 문제를 수정(db 쿼리로 검색, 데이터 전처리)
2. DB
- 필요한 데이터와 프론트 요청에 따라 테이블 및 뷰 수정
a. market의 chart데이터에 필요한 테이블 추가, 이를 조회할 수 있는 뷰 추가
b. 필요한 주가 데이터 기간 증가로 인해, ai 분석 요청시 조회하는 vw를 이에 대응하여 수정함
c. 프론트 요청에 따라 기존의 stock vw를 chart vw 로 추가 분리하고, 필요한 데이터만 표시하도록 변경
3. 협업
- ai 분석을 요청하는 클라이언트-소켓 간의 여러 문제 수정
a. request, response간 데이터 구조에 맞게 처리
b. 서버측의 압축 해제 실패로 인한 peer reset 오류 -> client는 byte stream 끝에 end를 추가하고, 서버는 loop로 확실하게 받도록 수정
c. 도커 간 dns 오류 -> 커스텀 네트워크에 연결되어있지 않던 문제 수정(기본 네트워크는 dns 지원 안함)
d. 분석에 필요한 데이터 요청을 받고 추가 수집 -> 기존 1일 데이터에서 한달 ~ 1년 데이터를 수집하여 분석 요청으로 수정
