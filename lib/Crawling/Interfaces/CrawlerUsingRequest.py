from .Crawler import CrawlerInterface
import requests
from bs4 import BeautifulSoup
import pandas as pd
from ..config.headers import HEADERS
from ..utils.save_data import save_to_json
from ..utils.random_delay import random_delay

class CrawlerUsingRequest(CrawlerInterface):
    def __init__(self, name, selector_config):
        super().__init__(name)
        self.tag = None
        self.config = selector_config
        self.max_articles = 2 # í¬ë¡¤ë§í•  ë‰´ìŠ¤ ìˆ˜
        self.max_retries = 30  # ìš”ì²­ ì¬ì‹œë„ íšŸìˆ˜

    # ì‚¬ì´íŠ¸ ë³´ì•ˆ ë°©ì‹ì— ë”°ë¼ ìì‹ í´ë˜ìŠ¤ì—ì„œ ì˜¤ë²„ë¼ì´ë“œ
    def fetch_page(self, url=None):
        if url is None:
            url = self.config["url"]

        retries = 0
        while retries < self.max_retries:
            try:
                response = requests.get(url, headers=HEADERS, timeout=20)
                response.raise_for_status()
                return {
                    "soup": BeautifulSoup(response.text, "html.parser"),
                    "status_code": response.status_code,
                    "url": url
                }

            except requests.exceptions.RequestException as e:
                print(f"[fetch_page] ìš”ì²­ ì‹¤íŒ¨: {e}")
                retries += 1
                if retries < self.max_retries:
                    print("Retrying...")
                    random_delay()

        return {
            "soup": None,
            "status_code": 500,
            "url": url
        }
        
    def crawl(self):
        """ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ ì •ë³´ ê°€ì ¸ì˜¤ê¸°"""
        # print(f"ğŸ” {self.__class__.__name__} í¬ë¡¤ë§ ì‹œì‘")
        fetch_result = self.fetch_page()
        soup = fetch_result["soup"]
        status_code = fetch_result["status_code"]
        target_url = fetch_result["url"]

        if not soup:
            return {
                "tag": self.tag,
                "log": {
                    "crawling_type": self.tag,
                    "status_code": status_code,
                    "target_url": target_url
                },
                "fail_log": {
                    "err_message": "HTML íŒŒì‹± ì‹¤íŒ¨ ë˜ëŠ” None ë°˜í™˜"
                }
            }

        try:
            articles = self.crawl_main(soup)
            if not articles:
                raise Exception("ê¸°ì‚¬ ì¶”ì¶œ ì‹¤íŒ¨ (crawl_main ê²°ê³¼ ì—†ìŒ)")

            df = pd.DataFrame(articles)
            return {
                "tag": self.tag,
                "log": {
                    "crawling_type": self.tag,
                    "status_code": status_code,
                    "target_url": target_url
                },
                "df": df
            }

        except Exception as e:
            return {
                "tag": self.tag,
                "log": {
                    "crawling_type": self.tag,
                    "status_code": 500,
                    "target_url": target_url
                },
                "fail_log": {
                    "err_message": str(e)
                }
            } 
    
    def crawl_main(self, soup):
        # print(f"ğŸ” {self.__class__.__name__} ë©”ì¸í˜ì´ì§€ í¬ë¡¤ë§ ì‹œì‘")
        # print(self.config)
        """ë©”ì¸ í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ ëª©ë¡ ì¶”ì¶œ"""
        articles, seen_urls = [], set()
        containers = self.extract_mainContainer(soup)
        # print(containers)

        if not containers:
            print("[ERROR] ë©”ì¸ ì»¨í…Œì´ë„ˆë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ! ì„ íƒì í™•ì¸ í•„ìš”")
            return None

        for article in containers:
            if len(articles) >= self.max_articles:
                break

            # âœ… JSONì—ì„œ ì •ì˜ëœ `main` í•„ë“œ ìë™ ì¶”ì¶œ
            main_data = self.extract_fields(article, "main")
            # print(main_data)

            url = self.get_absolute_url(main_data.get("href"))
            if not url or url in seen_urls:
                print("ì¤‘ë³µ ë§í¬ íƒì§€")
                continue
            seen_urls.add(url)

            # âœ… ê°œë³„ ê¸°ì‚¬ ì¶”ê°€ í¬ë¡¤ë§ ì‹¤í–‰
            article_content = self.crawl_content(url)
            if not article_content:
                print("ê¸°ì‚¬ ë‚´ìš© ì—†ìŒ")
                continue

            # âœ… JSONì—ì„œ ì •ì˜ëœ í•„ë“œ ê¸°ë°˜ìœ¼ë¡œ ë™ì  ë°ì´í„° ìƒì„±
            article_data = {
                **main_data,  # âœ… mainì—ì„œ ê°€ì ¸ì˜¨ ë°ì´í„° ì¶”ê°€
                **article_content  # âœ… contentì—ì„œ ê°€ì ¸ì˜¨ ë°ì´í„° ì¶”ê°€
            }
            articles.append(article_data)

        return articles

    def crawl_content(self, url):
        """ê¸°ì‚¬ ê°œë³„ í˜ì´ì§€ì—ì„œ ë³¸ë¬¸ ë° ì¶”ê°€ ì •ë³´ í¬ë¡¤ë§"""
        fetch_result = self.fetch_page(url)
        article_soup = fetch_result["soup"]

        if not article_soup:
            return None
        
        content_container = self.extract_contentContainer(article_soup)
        if not content_container:
            return None  # ì»¨í…ì¸  ì»¨í…Œì´ë„ˆê°€ ì—†ìœ¼ë©´ ê¸°ì‚¬ í¬ë¡¤ë§ ì‹¤íŒ¨
        
        article_content = self.extract_fields(content_container, "contents")

        return article_content
    
    def extract_mainContainer(self, soup):
        """ë©”ì¸ ë‰´ìŠ¤ ì»¨í…Œì´ë„ˆ ì¶”ì¶œ (ë‹¤ì¤‘ ì„ íƒì ì§€ì›)"""
        containers = []
        for selector in self.config["main_container_selectors"]:
            main_containers = soup.select(selector)
            if main_containers:
                containers.extend(main_containers)
        return containers if containers else None

    def extract_contentContainer(self, soup):
        """ê¸°ì‚¬ ë³¸ë¬¸ì´ í¬í•¨ëœ ìµœìƒìœ„ ì»¨í…Œì´ë„ˆ ì¶”ì¶œ"""
        for selector in self.config["content_container_selectors"]:
            content_container = soup.select_one(selector)
            if content_container:
                return content_container
        return None
    
    def extract_fields(self, soup, section):
        """JSON ì„¤ì •ì„ ê¸°ë°˜ìœ¼ë¡œ `extract_` í•¨ìˆ˜ ë™ì  í˜¸ì¶œ"""
        extracted_data = {}
        if section in self.config["selectors"]:
            for field, selectors in self.config["selectors"][section].items():
                extractor_func_name = f"extract_{field}"  # ì˜ˆ: extract_author, extract_content
                extractor_func = getattr(self, extractor_func_name, None)

                if callable(extractor_func):
                    extracted_data[field] = extractor_func(soup, selectors)

        return extracted_data

    def extract_href(self, soup, selectors):
        """ê¸°ì‚¬ ë§í¬ (href) ì¶”ì¶œ"""
        for selector in selectors:
            href_element = soup.select_one(selector)
            if href_element:
                return href_element["href"]
        return None

    def extract_organization(self, soup, selectors):
        """ê¸°ì‚¬ ì¶œì²˜ (organization) ì¶”ì¶œ"""
        for selector in selectors:
            organization_element = soup.select_one(selector)
            if organization_element:
                return organization_element.get_text(strip=True)
        return None

    def extract_author(self, soup, selectors):
        """ê¸°ì‚¬ ì‘ì„±ì (author) ì¶”ì¶œ"""
        for selector in selectors:
            author_element = soup.select_one(selector)
            if author_element:
                return author_element.get_text(strip=True)
        return "Unknown"

    def extract_title(self, soup, selectors):
        """ê¸°ì‚¬ ì œëª© (title) ì¶”ì¶œ"""
        for selector in selectors:
            title_element = soup.select_one(selector)
            if title_element:
                return title_element.get_text(strip=True)
        return None

    def extract_posted_at(self, soup, selectors):
        """ê¸°ì‚¬ ë‚ ì§œ (posted_at) ì¶”ì¶œ"""
        for selector in selectors:
            posted_at_element = soup.select_one(selector)
            if posted_at_element and posted_at_element.has_attr("datetime"):
                return posted_at_element["datetime"].split(" ")[0]
        return None

    def extract_content(self, soup, selectors):
        """ê¸°ì‚¬ ë³¸ë¬¸ (content) ì¶”ì¶œ"""
        content_texts = []
        for selector in selectors:
            content_elements = soup.select(selector)
            if content_elements:
                content_texts.extend([e.get_text(strip=True) for e in content_elements])
        return " ".join(content_texts).strip() if content_texts else None

    def extract_tag(self, soup, selectors):
        """ê´€ë ¨ ì£¼ì‹ (tag) ì¶”ì¶œ"""
        tag = []
        for selector in selectors:
            tag_elements = soup.select(selector)
            tag.extend([ticker.get_text(strip=True) for ticker in tag_elements])
        return tag if tag else None

    def get_absolute_url(self, url):
        """ì ˆëŒ€ URL ë³€í™˜"""
        return url if url.startswith("http") else self.config["base_url"] + url
