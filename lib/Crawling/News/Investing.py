from ..Interfaces.CrawlerUsingRequest import CrawlerUsingRequest
from ..config.headers import HEADERS
from ..utils.random_delay import random_delay
from bs4 import BeautifulSoup
import cloudscraper
import datetime
import re

class InvestingCrawler(CrawlerUsingRequest):

    def __init__(self, name, config):
        super().__init__(name, config)
        self.scraper = cloudscraper.create_scraper()
    
    """ ì˜¤ë²„ë¼ì´ë”© ì½”ë“œë“¤ """
    
    def crawl_content(self, url):
        """ê¸°ì‚¬ ê°œë³„ í˜ì´ì§€ì—ì„œ ë³¸ë¬¸ ë° ì¶”ê°€ ì •ë³´ í¬ë¡¤ë§"""
        article_soup = self.fetch_page(url, 10)

        if not article_soup:
            return None
        
        content_container = self.extract_contentContainer(article_soup)
        if not content_container:
            return None  # ì»¨í…ì¸  ì»¨í…Œì´ë„ˆê°€ ì—†ìœ¼ë©´ ê¸°ì‚¬ í¬ë¡¤ë§ ì‹¤íŒ¨
        
        article_content = self.extract_fields(content_container, "contents")

        return article_content

    def fetch_page(self, url=None, max_retries=None):
        """Cloudflare ìš°íšŒë¥¼ ìœ„í•œ í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸° (ìë™ ì¬ì‹œë„ í¬í•¨)"""
        if url is None:
            url = self.config["url"]

        if max_retries is None:
            max_retries = self.max_retries  # ê¸°ë³¸ ì„¤ì • ìœ ì§€

        retries = 0
        while retries < max_retries:
            try:
                # print(f"ğŸ” [ì‹œë„ {retries + 1}/{self.max_retries}] {url} ìš”ì²­ ì¤‘...")

                response = self.scraper.get(url, headers=HEADERS, timeout=20)
                
                # HTTP ì‘ë‹µ ì½”ë“œ í™•ì¸
                if response.status_code == 200:
                    # print(f"âœ… [ì‹œë„ {retries + 1}/{max_retries}] í¬ë¡¤ë§ ì„±ê³µ!")
                    return BeautifulSoup(response.text, "html.parser")

                # print(f"âš ï¸ [ì‹œë„ {retries + 1}/{max_retries}] HTTP ì˜¤ë¥˜ ë°œìƒ: {response.status_code}")
            
            except Exception as e:
                print(f"{self.__class__.__name__}: ìš”ì²­ ì‹¤íŒ¨ {e}")

            # ì¬ì‹œë„ ì„¤ì •
            retries += 1
            if retries < max_retries:
                # print("â³ Retry at intervals...")
                random_delay()

        print(f"{self.__class__.__name__}: ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨. url: {url}")
        return None
    
    def extract_mainContainer(self, soup):
        """ê²Œì‹œ ì‹œê°„ì´ ì—†ëŠ” ì»¨í…Œì´ë„ˆ ì œì™¸í•˜ê³  ë©”ì¸ ë‰´ìŠ¤ ì»¨í…Œì´ë„ˆ ì¶”ì¶œ"""
        containers = []
        for selector in self.config["main_container_selectors"]:
            main_containers = soup.select(selector)
            for container in main_containers:
                # <time> íƒœê·¸ê°€ ìˆëŠ”ì§€ í™•ì¸
                time_tag = container.select_one("time[data-test='article-publish-date']")
                if time_tag:
                    containers.append(container)  # ì‹œê°„ ì •ë³´ê°€ ìˆëŠ” ê²½ìš°ë§Œ ì¶”ê°€
        return containers if containers else None

    def extract_organization(self, soup, selectors):
        """ê¸°ì‚¬ ì¶œì²˜ (í¼ë¸”ë¦¬ì…”) ì¶”ì¶œ"""
        for selector in selectors:
            organization_element = soup.select_one(selector)
            if organization_element:
                return organization_element.find(text=True, recursive=False).strip()  # âœ… ì²« ë²ˆì§¸ í…ìŠ¤íŠ¸ ë…¸ë“œë§Œ ì¶”ì¶œ
        return None
    
    def extract_posted_at(self, soup, selectors):
        """ë‚ ì§œ ë¬¸ìì—´ì—ì„œ ì§ì ‘ ë‚ ì§œ ì¶”ì¶œ"""
        for selector in selectors:
            posted_at_element = soup.select_one(selector)
    
            if posted_at_element:
                date_text = posted_at_element.get_text(strip=True)
            
                # âœ… ì •ê·œì‹ìœ¼ë¡œ "MM/DD/YYYY" í˜•ì‹ ì¶”ì¶œ
                date_match = re.search(r"(\d{2}/\d{2}/\d{4})", date_text)
                if date_match:
                    extracted_date = date_match.group(1)
                    
                    # âœ… MM/DD/YYYY â†’ YYYY-MM-DD ë³€í™˜
                    formatted_date = datetime.datetime.strptime(extracted_date, "%m/%d/%Y").strftime("%Y-%m-%d")
                    return formatted_date

        return None  # ë‚ ì§œë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš°