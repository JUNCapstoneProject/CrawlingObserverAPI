"""
lib/Crawling/Stock/YFinanceStockCrawler.py
-----------------------------------------
* í•˜ë£¨ 1íšŒ: ì „ì¼ Adj Close + MarketCap ê³„ì‚°Â·ìºì‹±
* ë¶„ë´‰(15m) ë°ì´í„°ë§Œ ìˆ˜ì§‘ í›„ ìºì‹œëœ adj_close / market_cap ì£¼ì…
"""

from __future__ import annotations

import datetime
import json
import time
from pathlib import Path
from typing import List, Optional, Dict, Tuple
import threading, random
from curl_cffi import requests as curl_requests

session = curl_requests.Session(impersonate="chrome")

import logging
import pandas as pd
import requests
import yfinance as yf
from concurrent.futures import ThreadPoolExecutor, as_completed

from lib.Crawling.Interfaces.Crawler import CrawlerInterface
from lib.Distributor.secretary.models.company import Company
from lib.Distributor.secretary.session import get_session
from lib.Exceptions.exceptions import (
    BatchProcessingException,
    CrawlerException,
    DataNotFoundException,
)
from lib.Config.config import Config
from lib.Logger.logger import Logger
from lib.Crawling.utils.yfhandler import _YFForwardHandler

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ê³µí†µ ìœ í‹¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# def _get_us_market_date(now: datetime.datetime | None = None) -> str:
#     """UTC 22:00(=NY 17:00) ì´ì „ì´ë©´ ì–´ì œë¥¼, ì´í›„ë©´ ì˜¤ëŠ˜ ë‚ ì§œë¥¼ ë°˜í™˜ (yyyy-mm-dd)"""
#     now = now or datetime.datetime.utcnow()
#     cutoff = datetime.time(22, 0)
#     date = (
#         now.date() - datetime.timedelta(days=1) if now.time() < cutoff else now.date()
#     )
#     return date.isoformat()


def get_symbols_from_db(limit: int | None = None) -> List[str]:
    with get_session() as session:
        q = session.query(Company.ticker).order_by(Company.company_id.asc())
        if limit:
            q = q.limit(limit)
        return [t[0] for t in q.all()]


# â”€â”€â”€â”€â”€â”€â”€â”€â”€ Share ìºì‹œ íŒŒì¼ ê²½ë¡œ â”€â”€â”€â”€â”€â”€â”€â”€â”€
_SHARES_FILE = Path(__file__).with_name("shares_cache.json")


# shares_cache.json â‡’ {"_meta":{"last_reset":"2025-05"},"data":{"AAPL":...}}
def _load_shares_file() -> tuple[dict[str, str], dict[str, int]]:
    if _SHARES_FILE.exists():
        try:
            raw = json.loads(_SHARES_FILE.read_text())
            if "_meta" in raw and "data" in raw:
                return raw["_meta"], raw["data"]
            # êµ¬ë²„ì „(ë©”íƒ€ ì—†ìŒ) í˜¸í™˜
            return {}, raw
        except Exception:
            pass
    return {}, {}


def _dump_shares_file(meta: dict[str, str], cache: dict[str, int]) -> None:
    try:
        _SHARES_FILE.write_text(
            json.dumps({"_meta": meta, "data": cache}, separators=(",", ":"))
        )
    except Exception:
        pass


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë©”ì¸ í¬ë¡¤ëŸ¬
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class YFinanceStockCrawler(CrawlerInterface):
    """15 ë¶„ë´‰ OHLCV + ì „ì¼ Adj Close + MarketCap"""

    # í´ë˜ìŠ¤ ë ˆë²¨ ìºì‹œ
    _ticker_cik_map: dict[str, str] = {}  # {TICKER: CIK(10)}
    _cached_shares: dict[str, int] = {}  # {TICKER: shares}
    _cached_price_cap: dict[str, tuple[float, float, str]] = {}
    #                        â†³ (adj_close, market_cap, adj_date)

    def __init__(self, name: str):
        super().__init__(name)
        self.batch_size = 30
        self.max_workers = 10
        self.symbols = get_symbols_from_db(Config.get("symbol_size.total", 6000))
        self.tag = "stock"
        self.logger = Logger(self.__class__.__name__)

        # ğŸ”¹ yfinance ë¡œê·¸ â†’ ìš°ë¦¬ Logger ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸
        yf_logger = logging.getLogger("yfinance")
        yf_logger.handlers.clear()  # ê¸°ë³¸ ìŠ¤íŠ¸ë¦¼í•¸ë“¤ëŸ¬ ì œê±°
        yf_logger.setLevel(logging.INFO)  # ì›í•˜ëŠ” ìµœì†Œ ë ˆë²¨
        yf_logger.addHandler(_YFForwardHandler(self.logger))
        yf_logger.propagate = False  # ë£¨íŠ¸ ë¡œê±°ë¡œ ì „íŒŒ ë°©ì§€

        self._load_ticker_cik_map()
        self._shares_meta, self._cached_shares = _load_shares_file()

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Ticker-CIK ë§¤í•‘ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _load_ticker_cik_map(self):
        """
        cik2ticker.json â†’ {TICKER: CIK(10ìë¦¬)} ë¡œ ë³€í™˜
        - ë¦¬ìŠ¤íŠ¸ í˜•ì‹  : [{"ticker": "AAPL", "cik_str": "320193"}, ...]
        - ë”•ì…”ë„ˆë¦¬ í˜•ì‹: {"AAPL": "320193"}  or  {"AAPL": {"cik_str": "320193", ...}}
        """
        if self._ticker_cik_map:
            return

        path = Path(__file__).with_name("cik2ticker.json")
        try:
            with path.open(encoding="utf-8") as f:
                data = json.load(f)

            if isinstance(data, list):  # ğŸ“„ â‘  ë¦¬ìŠ¤íŠ¸
                for obj in data:
                    ticker = (obj.get("ticker") or obj.get("symbol", "")).upper()
                    cik_raw = str(obj.get("cik_str") or obj.get("CIK") or "").lstrip(
                        "0"
                    )
                    if ticker and cik_raw:
                        self._ticker_cik_map[ticker] = f"{int(cik_raw):010d}"

            elif isinstance(data, dict):  # ğŸ“„ â‘¡ ë”•ì…”ë„ˆë¦¬
                for ticker, val in data.items():
                    ticker = ticker.upper()
                    if isinstance(val, dict):  # {"AAPL": {"cik_str": "..."}}
                        cik_raw = str(
                            val.get("cik_str") or val.get("CIK") or val.get("cik") or ""
                        ).lstrip("0")
                    else:  # {"AAPL": "320193"}
                        cik_raw = str(val).lstrip("0")

                    if cik_raw:
                        self._ticker_cik_map[ticker] = f"{int(cik_raw):010d}"

            self.logger.log(
                "DEBUG", f"CIK ë§¤í•‘ ë¡œë“œ ì™„ë£Œ: {len(self._ticker_cik_map):,} tickers"
            )

        except Exception as e:
            self.logger.log("ERROR", f"CIK ë§¤í•‘ ë¡œë“œ ì‹¤íŒ¨: {e}")

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ë°œí–‰ì£¼ì‹ìˆ˜ ìºì‹œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _get_shares_outstanding(self, symbol: str) -> Optional[int]:
        """
        â€¢ ìºì‹œ â†’ ë‹¤ì¤‘ ì½˜ì…‰íŠ¸(US-GAAP, DEI, IFRS) ìˆœíšŒ â†’ Yahoo quote fallback
        â€¢ ì„±ê³µ ì‹œ _cached_shares[symbol] = shares
        """
        cached = self._cached_shares.get(symbol)
        if cached:
            return cached

        cik = self._ticker_cik_map.get(symbol.upper())
        if not cik:
            return None

        # 1ï¸âƒ£ SEC XBRL ì½˜ì…‰íŠ¸ í›„ë³´
        concept_paths = [
            ("us-gaap", "CommonStockSharesOutstanding"),
            ("dei", "EntityCommonStockSharesOutstanding"),
            ("us-gaap", "WeightedAverageNumberOfSharesOutstandingBasic"),
            ("ifrs-full", "SharesOutstanding"),  # â”€ IFRS filers
            ("ifrs-full", "OrdinarySharesNumber"),  # â”€ ì¼ë¶€ ADR
        ]

        headers = {"User-Agent": "StockCrawler/1.0 (contact: you@example.com)"}
        for taxonomy, concept in concept_paths:
            url = (
                f"https://data.sec.gov/api/xbrl/companyconcept/"
                f"CIK{cik}/{taxonomy}/{concept}.json"
            )
            try:
                r = requests.get(url, headers=headers, timeout=8)
                if r.status_code == 404:
                    continue  # ë‹¤ìŒ í›„ë³´ ì½˜ì…‰íŠ¸ ì‹œë„
                r.raise_for_status()
                data = r.json()
                units = data.get("units", {})
                # 'shares' or 'num' ë“± í‚¤ê°€ ë‹¤ì–‘ â†’ ëª¨ë“  list í•©ì¹˜ê¸°
                all_units = []
                for v in units.values():
                    all_units.extend(v)
                if not all_units:
                    continue

                latest = max(all_units, key=lambda x: x.get("end", ""))
                shares = int(latest.get("val", 0))
                if shares > 0:
                    # self.logger.log(
                    #     "DEBUG", f"{symbol} sharesOutstanding ì„±ê³µ"
                    # )  # í…ŒìŠ¤íŠ¸ìš© =============================================
                    self._cached_shares[symbol] = shares
                    return shares
            except Exception as e:
                # ë¡œê¹…ì€ DEBUG ìˆ˜ì¤€ìœ¼ë¡œë§Œ
                self.logger.log("DEBUG", f"{symbol} concept '{concept}' ì‹¤íŒ¨: {e}")

        # 2ï¸âƒ£ Fallback: Yahoo Finance quote?fields=sharesOutstanding
        try:
            tkr = yf.Ticker(symbol)

            # âœ´ï¸ fast_info ê°€ ê°€ì¥ ë¹ ë¦„ (0.05s ë¯¸ë§Œ)
            shares = None
            if hasattr(tkr, "fast_info"):
                shares = tkr.fast_info.get("sharesOutstanding")

            # fast_info ì— ì—†ìœ¼ë©´ .get_info() â†’ .info ìˆœ
            if not shares:
                try:
                    shares = tkr.get_info()["sharesOutstanding"]  # yfinance â‰¥0.2
                except AttributeError:
                    shares = tkr.info.get("sharesOutstanding")  # yfinance â‰¤0.1

            if shares:
                # self.logger.log(
                #     "DEBUG", f"{symbol} yfinance sharesOutstanding ì„±ê³µ"
                # )  # í…ŒìŠ¤íŠ¸ìš© =============================================
                shares = int(shares)
                self._cached_shares[symbol] = shares
                return shares

        except Exception as e:
            self.logger.log("DEBUG", f"{symbol} yfinance sharesOutstanding ì‹¤íŒ¨: {e}")

        # 3ï¸âƒ£ ìµœì¢… ì‹¤íŒ¨
        self.logger.log("WARN", f"{symbol}: ë°œí–‰ì£¼ì‹ìˆ˜ í™•ì¸ ì‹¤íŒ¨ (SEC+Yahoo ëª¨ë‘ ì‹¤íŒ¨)")
        return None

    def _bulk_fetch_shares(
        self, symbols: list[str], max_workers: int = 8
    ) -> dict[str, int]:
        """
        SEC XBRLì„ ë³‘ë ¬ í˜¸ì¶œ â†’ {ticker: shares}
        â€¢ ì´ˆë‹¹ 10ê±´ ì´í•˜ (token-bucket)
        â€¢ ê° í‹°ì»¤ì˜ ì‹œë„/ì„±ê³µ/ì‹¤íŒ¨ë¥¼ DEBUG ë¡œê·¸ë¡œ ì¶œë ¥
        """
        symbols = [s for s in symbols if s not in self._cached_shares]
        if not symbols:
            # self.logger.log("INFO", "[Shares] ëª¨ë‘ íŒŒì¼ ìºì‹œ")
            return {}

        session = requests.Session()
        headers = {"User-Agent": "StockCrawler/1.0 (contact: you@example.com)"}

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ token-bucket â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        tokens = threading.Semaphore(10)

        def refill():
            while True:
                time.sleep(1)
                for _ in range(10 - tokens._value):
                    tokens.release()

        threading.Thread(target=refill, daemon=True).start()

        concept_paths = [
            ("us-gaap", "CommonStockSharesOutstanding"),
            ("dei", "EntityCommonStockSharesOutstanding"),
            ("ifrs-full", "SharesOutstanding"),
            ("ifrs-full", "OrdinarySharesNumber"),
        ]

        def fetch_single(tkr: str) -> Optional[int]:
            cik = self._ticker_cik_map.get(tkr)
            if not cik:
                return None

            for tax, con in concept_paths:
                url = f"https://data.sec.gov/api/xbrl/companyconcept/CIK{cik}/{tax}/{con}.json"
                tokens.acquire()
                try:
                    r = session.get(url, headers=headers, timeout=6)
                    if r.status_code == 404:
                        continue
                    r.raise_for_status()

                    units = r.json().get("units", {})
                    flat = [u for v in units.values() for u in v]
                    if not flat:
                        continue

                    latest = max(flat, key=lambda x: x.get("end", ""))
                    val = int(latest.get("val", 0))
                    if val:
                        # self.logger.log(
                        #     "DEBUG", f"{tkr} shares âœ… SEC {tax}/{con}"
                        # )  # í…ŒìŠ¤íŠ¸ìš© =============================================
                        return val
                except Exception as e:
                    continue

            return None

        shares_map: dict[str, int] = {}
        with ThreadPoolExecutor(max_workers=max_workers) as ex:
            fut = {ex.submit(fetch_single, s): s for s in symbols}
            for f in as_completed(fut):
                sym = fut[f]
                val = f.result()
                if val:
                    shares_map[sym] = val
                    self._cached_shares[sym] = val  # ë©”ëª¨ë¦¬ ìºì‹œ ë™ì‹œ ê°±ì‹ 

        # self.logger.log(
        #     "INFO",
        #     f"[Shares] ë³‘ë ¬ fetch ì™„ë£Œ â–¶ ì„±ê³µ {len(shares_map):,} / ìš”ì²­ {len(symbols):,}",
        # )
        return shares_map

    def _ensure_price_cap_cache(self):
        """Adj Close & MarketCap ìºì‹œ ê°±ì‹  (curl + chart API)"""

        today = datetime.date.today()
        this_month = today.strftime("%Y-%m")

        if today.day == 2 and self._shares_meta.get("last_reset") != this_month:
            self.logger.log("INFO", "[Shares] ì›”ê°„ ë¦¬ì…‹ ì‹¤í–‰")
            self._cached_shares.clear()
            self._shares_meta["last_reset"] = this_month

        if not self.symbols:
            return

        # 1) sentinel ì‹¬ë³¼ë¡œ adj ë‚ ì§œ íŒŒì•…
        sentinel = self.symbols[0]
        try:
            sentinel_data = self._download_adjclose_chart([sentinel])
            if sentinel not in sentinel_data:
                self.logger.log("WARN", f"sentinel {sentinel} ë°ì´í„° ì—†ìŒ")
                return
            _, adj_date = sentinel_data[sentinel]
        except Exception as e:
            self.logger.log("ERROR", f"sentinel ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            return

        # 2) stale í‹°ì»¤ ì„ ë³„
        stale = [
            s
            for s in self.symbols
            if s not in self._cached_price_cap
            or self._cached_price_cap[s][2] != adj_date
        ]
        if not stale:
            self.logger.log("INFO", f"[MarketCap] ìºì‹œ ìµœì‹  (adj_date={adj_date})")
            return

        # 3) adj close ë‹¤ìš´ë¡œë“œ (chart API)
        self.logger.log("DEBUG", "Adj Close ë‹¤ìš´ë¡œë“œ ì‹œì‘")
        adjclose_map = self._download_adjclose_chart(stale)
        if not adjclose_map:
            self.logger.log("ERROR", "Adj Close ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨")
            return
        self.logger.log("DEBUG", "Adj Close ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")

        # 4) SEC ë³‘ë ¬ í˜¸ì¶œ â†’ ë°œí–‰ì£¼ì‹ìˆ˜
        shares_map = self._bulk_fetch_shares(stale)

        # 5) ë³‘í•©Â·ìºì‹±
        updated = 0
        for sym in stale:
            try:
                if sym not in adjclose_map:
                    continue
                adj_close, adj_date = adjclose_map[sym]

                shares = shares_map.get(sym) or self._get_shares_outstanding(sym)
                if not shares:
                    continue

                cap = adj_close * shares
                self._cached_price_cap[sym] = (adj_close, cap, adj_date)
                updated += 1
            except Exception as e:
                self.logger.log("DEBUG", f"{sym} ìºì‹± ì‹¤íŒ¨: {e}")

        _dump_shares_file(self._shares_meta, self._cached_shares)
        self.logger.log(
            "DEBUG",
            f"[MarketCap] ìºì‹œ ê°±ì‹  ì™„ë£Œ {updated:,}/{len(stale):,} (adj_date={adj_date})",
        )

    def _download_adjclose_chart(
        self,
        symbols: List[str],
        days: int = 7,
        max_workers: int = 10,
        batch_size: int = 20,
    ) -> Dict[str, Tuple[float, str]]:
        """yfinance Ticker().history()ë¡œ adj close + ë‚ ì§œ ë³‘ë ¬ ì¶”ì¶œ (10ë‹¨ìœ„ ë°°ì¹˜ ë””ë²„ê·¸ ë¡œê·¸)"""
        result = {}

        def fetch_single(sym: str):
            for _ in range(3):  # ìµœëŒ€ 3íšŒ ì¬ì‹œë„
                try:
                    time.sleep(random.uniform(0.1, 0.3))
                    ticker = yf.Ticker(sym)
                    df = ticker.history(
                        period=f"{days}d",
                        interval="1d",
                        auto_adjust=False,
                    )

                    if df.empty or "Adj Close" not in df.columns:
                        continue

                    last_row = df.dropna(subset=["Adj Close"]).iloc[-1]
                    adj_close = float(last_row["Adj Close"])
                    date = last_row.name.date()
                    return sym, (adj_close, str(date))

                except Exception as e:
                    self.logger.log("WARN", f"{sym} yfinance history ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
            return None

        for i in range(0, len(symbols), batch_size):
            batch_num = i // batch_size + 1
            if batch_num % 10 == 0:
                self.logger.log(
                    "DEBUG", f"{batch_num}ë²ˆì§¸ ë°°ì¹˜ ì‹¤í–‰ ì¤‘ (index {i}~{i+batch_size})"
                )

            batch = symbols[i : i + batch_size]

            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                futures = {executor.submit(fetch_single, sym): sym for sym in batch}
                for future in as_completed(futures):
                    result_item = future.result()
                    if result_item:
                        sym, value = result_item
                        result[sym] = value

        return result

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ public â–¶ crawl â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def crawl(self):
        # â‘  ì „ì¼ Adj Close & MarketCap ìºì‹œ í™•ë³´
        self._ensure_price_cap_cache()

        if not self.symbols:
            return None

        batches = [
            self.symbols[i : i + self.batch_size]
            for i in range(0, len(self.symbols), self.batch_size)
        ]
        results = []

        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            future_to_batch = {
                executor.submit(self._crawl_batch, batch, i): batch
                for i, batch in enumerate(batches)
            }

            for future in as_completed(future_to_batch):
                batch = future_to_batch[future]
                try:
                    results.extend(future.result())
                except BatchProcessingException as e:
                    for sym in batch:
                        results.append(
                            {
                                "tag": self.tag,
                                "log": {
                                    "crawling_type": self.tag,
                                    "status_code": e.status_code,
                                },
                                "fail_log": {"err_message": str(e)},
                            }
                        )
                except Exception as e:
                    for sym in batch:
                        results.append(
                            {
                                "tag": self.tag,
                                "log": {"crawling_type": self.tag, "status_code": 500},
                                "fail_log": {"err_message": f"batch ì˜¤ë¥˜: {e}"},
                            }
                        )

        return results

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ë¶„ë´‰ ë°ì´í„° ìˆ˜ì§‘ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def _crawl_batch(self, batch: List[str], batch_id: int):
        batch_results = []

        if batch_id % 10 == 0:
            self.logger.log("DEBUG", f"[ë°°ì¹˜ ì§„í–‰] [{batch_id}]ë²ˆì§¸ ë°°ì¹˜ ì‹œì‘")

        for symbol in batch:
            try:
                stock = yf.Ticker(symbol, session=session)

                # â–¶ adj_close / market_cap ìºì‹œ
                cache_item = self._cached_price_cap.get(symbol)
                if not cache_item:
                    self.logger.log("DEBUG", f"[{symbol}]ê°€ê²©/ì‹œì´ ìºì‹œ ëˆ„ë½")
                    raise DataNotFoundException("ê°€ê²©/ì‹œì´ ìºì‹œ ëˆ„ë½", source=symbol)
                adj_close, market_cap, _ = cache_item

                # âœ… df ê°€ê³µ
                df_min = self._process_minute_data(stock, symbol, adj_close, market_cap)

                batch_results.append(
                    {
                        "tag": self.tag,
                        "log": {"crawling_type": self.tag, "status_code": 200},
                        "df": df_min,
                    }
                )

            except CrawlerException as e:
                batch_results.append(
                    {
                        "tag": self.tag,
                        "log": {
                            "crawling_type": self.tag,
                            "status_code": e.status_code,
                        },
                        "fail_log": {"err_message": str(e)},
                    }
                )
            except Exception as e:
                batch_results.append(
                    {
                        "tag": self.tag,
                        "log": {"crawling_type": self.tag, "status_code": 500},
                        "fail_log": {
                            "err_message": f"{symbol} ì²˜ë¦¬ ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜: {str(e)}"
                        },
                    }
                )

            time.sleep(random.uniform(0.1, 0.4))

        return batch_results

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ë¶„ë´‰ â†’ 1í–‰ ë³€í™˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def _process_minute_data(
        self,
        stock,
        symbol: str,
        adj_close: Optional[float],
        market_cap: Optional[float],
    ) -> pd.DataFrame:

        df_min = stock.history(period="1d", interval="1m", prepost=True)[
            ["Open", "High", "Low", "Close", "Volume"]
        ]

        if df_min.empty:
            raise DataNotFoundException(
                "Empty DataFrame (ê±°ë˜ ë°ì´í„° ì—†ìŒ)", source=symbol
            )

        df_min = df_min.tail(1).reset_index()
        df_min.rename(columns={"Datetime": "posted_at"}, inplace=True)
        df_min["Symbol"] = symbol
        df_min["Adj Close"] = round(adj_close, 2)
        df_min["MarketCap"] = int(market_cap)

        return df_min
